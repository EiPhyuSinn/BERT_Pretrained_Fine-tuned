{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eps/Documents/Projects/Bert(Training_with_new_dataset)/bert_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BertTokenizer,\n",
    "    AutoConfig, \n",
    "    AutoModelForSequenceClassification,\n",
    "    BertForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer)\n",
    "\n",
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['textID', 'text', 'label', 'label_text'],\n",
       "        num_rows: 27481\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['textID', 'text', 'label', 'label_text'],\n",
       "        num_rows: 3534\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"SetFit/tweet_sentiment_extraction\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'textID': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'label': Value(dtype='int64', id=None), 'label_text': Value(dtype='string', id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(ds[\"train\"].features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping:\n",
      "   label label_text\n",
      "1      0   negative\n",
      "0      1    neutral\n",
      "6      2   positive\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['negative', 'neutral', 'positive']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.DataFrame(ds[\"train\"])\n",
    "\n",
    "label_mapping = train_df[[\"label\", \"label_text\"]].drop_duplicates().sort_values(\"label\")\n",
    "print(\"Label mapping:\")\n",
    "print(label_mapping)\n",
    "\n",
    "label_names = label_mapping[\"label_text\"].tolist()\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(ds['train']['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sentiment: LABEL_0\n"
     ]
    }
   ],
   "source": [
    "text = \"puppy is cute\"\n",
    "input = tokenizer(text,return_tensors='pt',padding=True, truncation=True,max_length =512)\n",
    "output = model(**input)\n",
    "predicted_class = torch.argmax(output.logits).item()\n",
    "print(f\"Predicted Sentiment: {model.config.id2label[predicted_class]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "bert\n",
      "bert.embeddings\n",
      "bert.embeddings.word_embeddings\n",
      "bert.embeddings.position_embeddings\n",
      "bert.embeddings.token_type_embeddings\n",
      "bert.embeddings.LayerNorm\n",
      "bert.embeddings.dropout\n",
      "bert.encoder\n",
      "bert.encoder.layer\n",
      "bert.encoder.layer.0\n",
      "bert.encoder.layer.0.attention\n",
      "bert.encoder.layer.0.attention.self\n",
      "bert.encoder.layer.0.attention.self.query\n",
      "bert.encoder.layer.0.attention.self.key\n",
      "bert.encoder.layer.0.attention.self.value\n",
      "bert.encoder.layer.0.attention.self.dropout\n",
      "bert.encoder.layer.0.attention.output\n",
      "bert.encoder.layer.0.attention.output.dense\n",
      "bert.encoder.layer.0.attention.output.LayerNorm\n",
      "bert.encoder.layer.0.attention.output.dropout\n",
      "bert.encoder.layer.0.intermediate\n",
      "bert.encoder.layer.0.intermediate.dense\n",
      "bert.encoder.layer.0.intermediate.intermediate_act_fn\n",
      "bert.encoder.layer.0.output\n",
      "bert.encoder.layer.0.output.dense\n",
      "bert.encoder.layer.0.output.LayerNorm\n",
      "bert.encoder.layer.0.output.dropout\n",
      "bert.encoder.layer.1\n",
      "bert.encoder.layer.1.attention\n",
      "bert.encoder.layer.1.attention.self\n",
      "bert.encoder.layer.1.attention.self.query\n",
      "bert.encoder.layer.1.attention.self.key\n",
      "bert.encoder.layer.1.attention.self.value\n",
      "bert.encoder.layer.1.attention.self.dropout\n",
      "bert.encoder.layer.1.attention.output\n",
      "bert.encoder.layer.1.attention.output.dense\n",
      "bert.encoder.layer.1.attention.output.LayerNorm\n",
      "bert.encoder.layer.1.attention.output.dropout\n",
      "bert.encoder.layer.1.intermediate\n",
      "bert.encoder.layer.1.intermediate.dense\n",
      "bert.encoder.layer.1.intermediate.intermediate_act_fn\n",
      "bert.encoder.layer.1.output\n",
      "bert.encoder.layer.1.output.dense\n",
      "bert.encoder.layer.1.output.LayerNorm\n",
      "bert.encoder.layer.1.output.dropout\n",
      "bert.encoder.layer.2\n",
      "bert.encoder.layer.2.attention\n",
      "bert.encoder.layer.2.attention.self\n",
      "bert.encoder.layer.2.attention.self.query\n",
      "bert.encoder.layer.2.attention.self.key\n",
      "bert.encoder.layer.2.attention.self.value\n",
      "bert.encoder.layer.2.attention.self.dropout\n",
      "bert.encoder.layer.2.attention.output\n",
      "bert.encoder.layer.2.attention.output.dense\n",
      "bert.encoder.layer.2.attention.output.LayerNorm\n",
      "bert.encoder.layer.2.attention.output.dropout\n",
      "bert.encoder.layer.2.intermediate\n",
      "bert.encoder.layer.2.intermediate.dense\n",
      "bert.encoder.layer.2.intermediate.intermediate_act_fn\n",
      "bert.encoder.layer.2.output\n",
      "bert.encoder.layer.2.output.dense\n",
      "bert.encoder.layer.2.output.LayerNorm\n",
      "bert.encoder.layer.2.output.dropout\n",
      "bert.encoder.layer.3\n",
      "bert.encoder.layer.3.attention\n",
      "bert.encoder.layer.3.attention.self\n",
      "bert.encoder.layer.3.attention.self.query\n",
      "bert.encoder.layer.3.attention.self.key\n",
      "bert.encoder.layer.3.attention.self.value\n",
      "bert.encoder.layer.3.attention.self.dropout\n",
      "bert.encoder.layer.3.attention.output\n",
      "bert.encoder.layer.3.attention.output.dense\n",
      "bert.encoder.layer.3.attention.output.LayerNorm\n",
      "bert.encoder.layer.3.attention.output.dropout\n",
      "bert.encoder.layer.3.intermediate\n",
      "bert.encoder.layer.3.intermediate.dense\n",
      "bert.encoder.layer.3.intermediate.intermediate_act_fn\n",
      "bert.encoder.layer.3.output\n",
      "bert.encoder.layer.3.output.dense\n",
      "bert.encoder.layer.3.output.LayerNorm\n",
      "bert.encoder.layer.3.output.dropout\n",
      "bert.encoder.layer.4\n",
      "bert.encoder.layer.4.attention\n",
      "bert.encoder.layer.4.attention.self\n",
      "bert.encoder.layer.4.attention.self.query\n",
      "bert.encoder.layer.4.attention.self.key\n",
      "bert.encoder.layer.4.attention.self.value\n",
      "bert.encoder.layer.4.attention.self.dropout\n",
      "bert.encoder.layer.4.attention.output\n",
      "bert.encoder.layer.4.attention.output.dense\n",
      "bert.encoder.layer.4.attention.output.LayerNorm\n",
      "bert.encoder.layer.4.attention.output.dropout\n",
      "bert.encoder.layer.4.intermediate\n",
      "bert.encoder.layer.4.intermediate.dense\n",
      "bert.encoder.layer.4.intermediate.intermediate_act_fn\n",
      "bert.encoder.layer.4.output\n",
      "bert.encoder.layer.4.output.dense\n",
      "bert.encoder.layer.4.output.LayerNorm\n",
      "bert.encoder.layer.4.output.dropout\n",
      "bert.encoder.layer.5\n",
      "bert.encoder.layer.5.attention\n",
      "bert.encoder.layer.5.attention.self\n",
      "bert.encoder.layer.5.attention.self.query\n",
      "bert.encoder.layer.5.attention.self.key\n",
      "bert.encoder.layer.5.attention.self.value\n",
      "bert.encoder.layer.5.attention.self.dropout\n",
      "bert.encoder.layer.5.attention.output\n",
      "bert.encoder.layer.5.attention.output.dense\n",
      "bert.encoder.layer.5.attention.output.LayerNorm\n",
      "bert.encoder.layer.5.attention.output.dropout\n",
      "bert.encoder.layer.5.intermediate\n",
      "bert.encoder.layer.5.intermediate.dense\n",
      "bert.encoder.layer.5.intermediate.intermediate_act_fn\n",
      "bert.encoder.layer.5.output\n",
      "bert.encoder.layer.5.output.dense\n",
      "bert.encoder.layer.5.output.LayerNorm\n",
      "bert.encoder.layer.5.output.dropout\n",
      "bert.encoder.layer.6\n",
      "bert.encoder.layer.6.attention\n",
      "bert.encoder.layer.6.attention.self\n",
      "bert.encoder.layer.6.attention.self.query\n",
      "bert.encoder.layer.6.attention.self.key\n",
      "bert.encoder.layer.6.attention.self.value\n",
      "bert.encoder.layer.6.attention.self.dropout\n",
      "bert.encoder.layer.6.attention.output\n",
      "bert.encoder.layer.6.attention.output.dense\n",
      "bert.encoder.layer.6.attention.output.LayerNorm\n",
      "bert.encoder.layer.6.attention.output.dropout\n",
      "bert.encoder.layer.6.intermediate\n",
      "bert.encoder.layer.6.intermediate.dense\n",
      "bert.encoder.layer.6.intermediate.intermediate_act_fn\n",
      "bert.encoder.layer.6.output\n",
      "bert.encoder.layer.6.output.dense\n",
      "bert.encoder.layer.6.output.LayerNorm\n",
      "bert.encoder.layer.6.output.dropout\n",
      "bert.encoder.layer.7\n",
      "bert.encoder.layer.7.attention\n",
      "bert.encoder.layer.7.attention.self\n",
      "bert.encoder.layer.7.attention.self.query\n",
      "bert.encoder.layer.7.attention.self.key\n",
      "bert.encoder.layer.7.attention.self.value\n",
      "bert.encoder.layer.7.attention.self.dropout\n",
      "bert.encoder.layer.7.attention.output\n",
      "bert.encoder.layer.7.attention.output.dense\n",
      "bert.encoder.layer.7.attention.output.LayerNorm\n",
      "bert.encoder.layer.7.attention.output.dropout\n",
      "bert.encoder.layer.7.intermediate\n",
      "bert.encoder.layer.7.intermediate.dense\n",
      "bert.encoder.layer.7.intermediate.intermediate_act_fn\n",
      "bert.encoder.layer.7.output\n",
      "bert.encoder.layer.7.output.dense\n",
      "bert.encoder.layer.7.output.LayerNorm\n",
      "bert.encoder.layer.7.output.dropout\n",
      "bert.encoder.layer.8\n",
      "bert.encoder.layer.8.attention\n",
      "bert.encoder.layer.8.attention.self\n",
      "bert.encoder.layer.8.attention.self.query\n",
      "bert.encoder.layer.8.attention.self.key\n",
      "bert.encoder.layer.8.attention.self.value\n",
      "bert.encoder.layer.8.attention.self.dropout\n",
      "bert.encoder.layer.8.attention.output\n",
      "bert.encoder.layer.8.attention.output.dense\n",
      "bert.encoder.layer.8.attention.output.LayerNorm\n",
      "bert.encoder.layer.8.attention.output.dropout\n",
      "bert.encoder.layer.8.intermediate\n",
      "bert.encoder.layer.8.intermediate.dense\n",
      "bert.encoder.layer.8.intermediate.intermediate_act_fn\n",
      "bert.encoder.layer.8.output\n",
      "bert.encoder.layer.8.output.dense\n",
      "bert.encoder.layer.8.output.LayerNorm\n",
      "bert.encoder.layer.8.output.dropout\n",
      "bert.encoder.layer.9\n",
      "bert.encoder.layer.9.attention\n",
      "bert.encoder.layer.9.attention.self\n",
      "bert.encoder.layer.9.attention.self.query\n",
      "bert.encoder.layer.9.attention.self.key\n",
      "bert.encoder.layer.9.attention.self.value\n",
      "bert.encoder.layer.9.attention.self.dropout\n",
      "bert.encoder.layer.9.attention.output\n",
      "bert.encoder.layer.9.attention.output.dense\n",
      "bert.encoder.layer.9.attention.output.LayerNorm\n",
      "bert.encoder.layer.9.attention.output.dropout\n",
      "bert.encoder.layer.9.intermediate\n",
      "bert.encoder.layer.9.intermediate.dense\n",
      "bert.encoder.layer.9.intermediate.intermediate_act_fn\n",
      "bert.encoder.layer.9.output\n",
      "bert.encoder.layer.9.output.dense\n",
      "bert.encoder.layer.9.output.LayerNorm\n",
      "bert.encoder.layer.9.output.dropout\n",
      "bert.encoder.layer.10\n",
      "bert.encoder.layer.10.attention\n",
      "bert.encoder.layer.10.attention.self\n",
      "bert.encoder.layer.10.attention.self.query\n",
      "bert.encoder.layer.10.attention.self.key\n",
      "bert.encoder.layer.10.attention.self.value\n",
      "bert.encoder.layer.10.attention.self.dropout\n",
      "bert.encoder.layer.10.attention.output\n",
      "bert.encoder.layer.10.attention.output.dense\n",
      "bert.encoder.layer.10.attention.output.LayerNorm\n",
      "bert.encoder.layer.10.attention.output.dropout\n",
      "bert.encoder.layer.10.intermediate\n",
      "bert.encoder.layer.10.intermediate.dense\n",
      "bert.encoder.layer.10.intermediate.intermediate_act_fn\n",
      "bert.encoder.layer.10.output\n",
      "bert.encoder.layer.10.output.dense\n",
      "bert.encoder.layer.10.output.LayerNorm\n",
      "bert.encoder.layer.10.output.dropout\n",
      "bert.encoder.layer.11\n",
      "bert.encoder.layer.11.attention\n",
      "bert.encoder.layer.11.attention.self\n",
      "bert.encoder.layer.11.attention.self.query\n",
      "bert.encoder.layer.11.attention.self.key\n",
      "bert.encoder.layer.11.attention.self.value\n",
      "bert.encoder.layer.11.attention.self.dropout\n",
      "bert.encoder.layer.11.attention.output\n",
      "bert.encoder.layer.11.attention.output.dense\n",
      "bert.encoder.layer.11.attention.output.LayerNorm\n",
      "bert.encoder.layer.11.attention.output.dropout\n",
      "bert.encoder.layer.11.intermediate\n",
      "bert.encoder.layer.11.intermediate.dense\n",
      "bert.encoder.layer.11.intermediate.intermediate_act_fn\n",
      "bert.encoder.layer.11.output\n",
      "bert.encoder.layer.11.output.dense\n",
      "bert.encoder.layer.11.output.LayerNorm\n",
      "bert.encoder.layer.11.output.dropout\n",
      "bert.pooler\n",
      "bert.pooler.dense\n",
      "bert.pooler.activation\n",
      "dropout\n",
      "classifier\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "                    task_type = 'SEQ_CLS',\n",
    "                    r = 3,\n",
    "                    lora_alpha = 42,\n",
    "                    lora_dropout = 0.01,\n",
    "                    target_modules = [\"query\", \"value\"])\n",
    "model = get_peft_model(model,peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LoraConfig in module peft.tuners.lora.config:\n",
      "\n",
      "class LoraConfig(peft.config.PeftConfig)\n",
      " |  LoraConfig(task_type: Union[str, peft.utils.peft_types.TaskType, NoneType] = None, peft_type: Union[str, peft.utils.peft_types.PeftType, NoneType] = None, auto_mapping: Optional[dict] = None, base_model_name_or_path: Optional[str] = None, revision: Optional[str] = None, inference_mode: bool = False, r: 'int' = 8, target_modules: 'Optional[Union[list[str], str]]' = None, exclude_modules: 'Optional[Union[list[str], str]]' = None, lora_alpha: 'int' = 8, lora_dropout: 'float' = 0.0, fan_in_fan_out: 'bool' = False, bias: \"Literal['none', 'all', 'lora_only']\" = 'none', use_rslora: 'bool' = False, modules_to_save: 'Optional[list[str]]' = None, init_lora_weights: \"bool | Literal['gaussian', 'eva', 'olora', 'pissa', 'pissa_niter_[number of iters]', 'corda', 'loftq']\" = True, layers_to_transform: 'Optional[Union[list[int], int]]' = None, layers_pattern: 'Optional[Union[list[str], str]]' = None, rank_pattern: 'Optional[dict]' = <factory>, alpha_pattern: 'Optional[dict]' = <factory>, megatron_config: 'Optional[dict]' = None, megatron_core: 'Optional[str]' = 'megatron.core', trainable_token_indices: 'Optional[Union[list[int], dict[str, list[int]]]]' = None, loftq_config: 'Union[LoftQConfig, dict]' = <factory>, eva_config: 'Optional[EvaConfig]' = None, corda_config: 'Optional[CordaConfig]' = None, use_dora: 'bool' = False, layer_replication: 'Optional[list[tuple[int, int]]]' = None, runtime_config: 'LoraRuntimeConfig' = <factory>, lora_bias: 'bool' = False) -> None\n",
      " |\n",
      " |  This is the configuration class to store the configuration of a [`LoraModel`].\n",
      " |\n",
      " |  Args:\n",
      " |      r (`int`):\n",
      " |          Lora attention dimension (the \"rank\").\n",
      " |      target_modules (`Optional[Union[List[str], str]]`):\n",
      " |          The names of the modules to apply the adapter to. If this is specified, only the modules with the specified\n",
      " |          names will be replaced. When passing a string, a regex match will be performed. When passing a list of\n",
      " |          strings, either an exact match will be performed or it is checked if the name of the module ends with any\n",
      " |          of the passed strings. If this is specified as 'all-linear', then all linear/Conv1D modules are chosen (if\n",
      " |          the model is a PreTrainedModel, the output layer excluded). If this is not specified, modules will be\n",
      " |          chosen according to the model architecture. If the architecture is not known, an error will be raised -- in\n",
      " |          this case, you should specify the target modules manually.\n",
      " |      exclude_modules (`Optional[Union[List[str], str]]`):\n",
      " |          The names of the modules to not apply the adapter. When passing a string, a regex match will be performed.\n",
      " |          When passing a list of strings, either an exact match will be performed or it is checked if the name of the\n",
      " |          module ends with any of the passed strings.\n",
      " |      lora_alpha (`int`):\n",
      " |          The alpha parameter for Lora scaling.\n",
      " |      lora_dropout (`float`):\n",
      " |          The dropout probability for Lora layers.\n",
      " |      fan_in_fan_out (`bool`):\n",
      " |          Set this to True if the layer to replace stores weight like (fan_in, fan_out). For example, gpt-2 uses\n",
      " |          `Conv1D` which stores weights like (fan_in, fan_out) and hence this should be set to `True`.\n",
      " |      bias (`str`):\n",
      " |          Bias type for LoRA. Can be 'none', 'all' or 'lora_only'. If 'all' or 'lora_only', the corresponding biases\n",
      " |          will be updated during training. Be aware that this means that, even when disabling the adapters, the model\n",
      " |          will not produce the same output as the base model would have without adaptation.\n",
      " |      use_rslora (`bool`):\n",
      " |          When set to True, uses <a href='https://doi.org/10.48550/arXiv.2312.03732'>Rank-Stabilized LoRA</a> which\n",
      " |          sets the adapter scaling factor to `lora_alpha/math.sqrt(r)`, since it was proven to work better.\n",
      " |          Otherwise, it will use the original default value of `lora_alpha/r`.\n",
      " |      modules_to_save (`List[str]`):\n",
      " |          List of modules apart from adapter layers to be set as trainable and saved in the final checkpoint.\n",
      " |      init_lora_weights (`bool` | `Literal[\"gaussian\", \"eva\", \"olora\", \"pissa\", \"pissa_niter_[number of iters]\", \"corda\", \"loftq\"]`):\n",
      " |          How to initialize the weights of the adapter layers. Passing True (default) results in the default\n",
      " |          initialization from the reference implementation from Microsoft, with the LoRA B weight being set to 0.\n",
      " |          This means that without further training, the LoRA adapter will be a no-op. Setting the initialization to\n",
      " |          False leads to random initialization of LoRA A and B, meaning that LoRA is not a no-op before training;\n",
      " |          this setting is intended for debugging purposes. Passing 'gaussian' results in Gaussian initialization\n",
      " |          scaled by the LoRA rank for linear and layers. Pass `'loftq'` to use LoftQ initialization. Passing `'eva'`\n",
      " |          results in a data-driven initialization of <ahref='https://arxiv.org/abs/2410.07170' >Explained Variance\n",
      " |          Adaptation</a>. EVA initalizes LoRA based on the SVD of layer input activations and achieves SOTA\n",
      " |          performance due to its ability to adapt to the finetuning data. Pass `'olora'` to use OLoRA initialization.\n",
      " |          Passing `'pissa'` results in the initialization of <ahref='https://arxiv.org/abs/2404.02948' >Principal\n",
      " |          Singular values and Singular vectors Adaptation (PiSSA)</a>, which converges more rapidly than LoRA and\n",
      " |          ultimately achieves superior performance. Moreover, PiSSA reduces the quantization error compared to QLoRA,\n",
      " |          leading to further enhancements. Passing `'pissa_niter_[number of iters]'` initiates Fast-SVD-based PiSSA\n",
      " |          initialization, where `[number of iters]` indicates the number of subspace iterations to perform FSVD, and\n",
      " |          must be a nonnegative integer. When `[number of iters]` is set to 16, it can complete the initialization of\n",
      " |          a 7B model within seconds, and the training effect is approximately equivalent to using SVD. Passing\n",
      " |          `'corda'` results in the initialization of <ahref='https://arxiv.org/abs/2406.05223' >Context-Oriented\n",
      " |          Decomposition Adaptation</a>, which converges even more rapidly than PiSSA in Instruction-Previewed Mode,\n",
      " |          and preserves world knowledge better than LoRA in Knowledge-Preserved Mode.\n",
      " |      layers_to_transform (`Union[List[int], int]`):\n",
      " |          The layer indices to transform. If a list of ints is passed, it will apply the adapter to the layer indices\n",
      " |          that are specified in this list. If a single integer is passed, it will apply the transformations on the\n",
      " |          layer at this index.\n",
      " |      layers_pattern (`Optional[Union[List[str], str]]`):\n",
      " |          The layer pattern name, used only if `layers_to_transform` is different from `None`. This should target the\n",
      " |          `nn.ModuleList` of the model, which is often called `'layers'` or `'h'`.\n",
      " |      rank_pattern (`dict`):\n",
      " |          The mapping from layer names or regexp expression to ranks which are different from the default rank\n",
      " |          specified by `r`. For example, `{'^model.decoder.layers.0.encoder_attn.k_proj': 16}`.\n",
      " |      alpha_pattern (`dict`):\n",
      " |          The mapping from layer names or regexp expression to alphas which are different from the default alpha\n",
      " |          specified by `lora_alpha`. For example, `{'^model.decoder.layers.0.encoder_attn.k_proj': 16}`.\n",
      " |      megatron_config (`Optional[dict]`):\n",
      " |          The TransformerConfig arguments for Megatron. It is used to create LoRA's parallel linear layer. You can\n",
      " |          get it like this, `core_transformer_config_from_args(get_args())`, these two functions being from Megatron.\n",
      " |          The arguments will be used to initialize the TransformerConfig of Megatron. You need to specify this\n",
      " |          parameter when you want to apply LoRA to the ColumnParallelLinear and RowParallelLinear layers of megatron.\n",
      " |      megatron_core (`Optional[str]`):\n",
      " |          The core module from Megatron to use, defaults to `\"megatron.core\"`.\n",
      " |      trainable_token_indices (`Optional[Union[List[int], dict[str, List[int]]]]`)\n",
      " |          Lets you specify which token indices to selectively fine-tune without requiring to re-train the whole\n",
      " |          embedding matrix using the `peft.TrainableTokensModel` method. You can specify token indices in two ways.\n",
      " |          Either you specify a list of indices which will then target the model's input embedding layer (or, if not\n",
      " |          found, `embed_tokens`). Alternatively, you can specify a dictionary where the key is the name of the\n",
      " |          embedding module and the values are the list of token indices, e.g. `{'embed_tokens': [0, 1, ...]}`. Note\n",
      " |          that training with FSDP/DeepSpeed might not yet be fully supported with this option enabled.\n",
      " |      loftq_config (`Optional[LoftQConfig]`):\n",
      " |          The configuration of LoftQ. If this is not None, then LoftQ will be used to quantize the backbone weights\n",
      " |          and initialize Lora layers. Also pass `init_lora_weights='loftq'`. Note that you should not pass a\n",
      " |          quantized model in this case, as LoftQ will quantize the model itself.\n",
      " |      eva_config (`Optional[EvaConfig]`):\n",
      " |          The configuration of EVA. At a minimum the dataset argument needs to be set (use the same dataset as for\n",
      " |          finetuning).\n",
      " |      corda_config (`Optional[CordaConfig]`):\n",
      " |          The configuration of CorDA. If this is not None, then CorDA will be used to build the adapter layers. Also\n",
      " |          pass `init_lora_weights='corda'`.\n",
      " |      use_dora (`bool`):\n",
      " |          Enable 'Weight-Decomposed Low-Rank Adaptation' (DoRA). This technique decomposes the updates of the weights\n",
      " |          into two parts, magnitude and direction. Direction is handled by normal LoRA, whereas the magnitude is\n",
      " |          handled by a separate learnable parameter. This can improve the performance of LoRA especially at low\n",
      " |          ranks. Right now, DoRA only supports linear and Conv2D layers. DoRA introduces a bigger overhead than pure\n",
      " |          LoRA, so it is recommended to merge weights for inference. For more information, see\n",
      " |          https://arxiv.org/abs/2402.09353.\n",
      " |      layer_replication (`List[Tuple[int, int]]`):\n",
      " |          Build a new stack of layers by stacking the original model layers according to the ranges specified. This\n",
      " |          allows expanding (or shrinking) the model without duplicating the base model weights. The new layers will\n",
      " |          all have separate LoRA adapters attached to them.\n",
      " |      runtime_config (`LoraRuntimeConfig`):\n",
      " |          Runtime configurations (which are not saved or restored).\n",
      " |      lora_bias (`bool`):\n",
      " |          Defaults to `False`. Whether to enable the bias term for the LoRA B parameter. Typically, this should be\n",
      " |          disabled. The main use case for this is when the LoRA weights were extracted from fully fine-tuned\n",
      " |          parameters so the bias of those parameters can be taken into account.\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      LoraConfig\n",
      " |      peft.config.PeftConfig\n",
      " |      peft.config.PeftConfigMixin\n",
      " |      transformers.utils.hub.PushToHubMixin\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |\n",
      " |  __init__(self, task_type: Union[str, peft.utils.peft_types.TaskType, NoneType] = None, peft_type: Union[str, peft.utils.peft_types.PeftType, NoneType] = None, auto_mapping: Optional[dict] = None, base_model_name_or_path: Optional[str] = None, revision: Optional[str] = None, inference_mode: bool = False, r: 'int' = 8, target_modules: 'Optional[Union[list[str], str]]' = None, exclude_modules: 'Optional[Union[list[str], str]]' = None, lora_alpha: 'int' = 8, lora_dropout: 'float' = 0.0, fan_in_fan_out: 'bool' = False, bias: \"Literal['none', 'all', 'lora_only']\" = 'none', use_rslora: 'bool' = False, modules_to_save: 'Optional[list[str]]' = None, init_lora_weights: \"bool | Literal['gaussian', 'eva', 'olora', 'pissa', 'pissa_niter_[number of iters]', 'corda', 'loftq']\" = True, layers_to_transform: 'Optional[Union[list[int], int]]' = None, layers_pattern: 'Optional[Union[list[str], str]]' = None, rank_pattern: 'Optional[dict]' = <factory>, alpha_pattern: 'Optional[dict]' = <factory>, megatron_config: 'Optional[dict]' = None, megatron_core: 'Optional[str]' = 'megatron.core', trainable_token_indices: 'Optional[Union[list[int], dict[str, list[int]]]]' = None, loftq_config: 'Union[LoftQConfig, dict]' = <factory>, eva_config: 'Optional[EvaConfig]' = None, corda_config: 'Optional[CordaConfig]' = None, use_dora: 'bool' = False, layer_replication: 'Optional[list[tuple[int, int]]]' = None, runtime_config: 'LoraRuntimeConfig' = <factory>, lora_bias: 'bool' = False) -> None\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  __post_init__(self)\n",
      " |\n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |\n",
      " |  to_dict(self)\n",
      " |      Returns the configuration for your adapter model as a dictionary. Removes runtime configurations.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __annotations__ = {'alpha_pattern': 'Optional[dict]', 'bias': \"Literal...\n",
      " |\n",
      " |  __dataclass_fields__ = {'alpha_pattern': Field(name='alpha_pattern',ty...\n",
      " |\n",
      " |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n",
      " |\n",
      " |  __hash__ = None\n",
      " |\n",
      " |  __match_args__ = ('task_type', 'peft_type', 'auto_mapping', 'base_mode...\n",
      " |\n",
      " |  bias = 'none'\n",
      " |\n",
      " |  corda_config = None\n",
      " |\n",
      " |  eva_config = None\n",
      " |\n",
      " |  exclude_modules = None\n",
      " |\n",
      " |  fan_in_fan_out = False\n",
      " |\n",
      " |  init_lora_weights = True\n",
      " |\n",
      " |  layer_replication = None\n",
      " |\n",
      " |  layers_pattern = None\n",
      " |\n",
      " |  layers_to_transform = None\n",
      " |\n",
      " |  lora_alpha = 8\n",
      " |\n",
      " |  lora_bias = False\n",
      " |\n",
      " |  lora_dropout = 0.0\n",
      " |\n",
      " |  megatron_config = None\n",
      " |\n",
      " |  megatron_core = 'megatron.core'\n",
      " |\n",
      " |  modules_to_save = None\n",
      " |\n",
      " |  r = 8\n",
      " |\n",
      " |  target_modules = None\n",
      " |\n",
      " |  trainable_token_indices = None\n",
      " |\n",
      " |  use_dora = False\n",
      " |\n",
      " |  use_rslora = False\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from peft.config.PeftConfig:\n",
      " |\n",
      " |  base_model_name_or_path = None\n",
      " |\n",
      " |  inference_mode = False\n",
      " |\n",
      " |  peft_type = None\n",
      " |\n",
      " |  revision = None\n",
      " |\n",
      " |  task_type = None\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from peft.config.PeftConfigMixin:\n",
      " |\n",
      " |  save_pretrained(self, save_directory: str, **kwargs) -> None\n",
      " |      This method saves the configuration of your adapter model in a directory.\n",
      " |\n",
      " |      Args:\n",
      " |          save_directory (`str`):\n",
      " |              The directory where the configuration will be saved.\n",
      " |          kwargs (additional keyword arguments, *optional*):\n",
      " |              Additional keyword arguments passed along to the [`~transformers.utils.PushToHubMixin.push_to_hub`]\n",
      " |              method.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from peft.config.PeftConfigMixin:\n",
      " |\n",
      " |  check_kwargs(**kwargs) from builtins.type\n",
      " |      Check kwargs before initializing the config instance.\n",
      " |\n",
      " |      Subclasses can override this method to add specific checks.\n",
      " |\n",
      " |  from_json_file(path_json_file: str, **kwargs) from builtins.type\n",
      " |      Loads a configuration file from a json file.\n",
      " |\n",
      " |      Args:\n",
      " |          path_json_file (`str`):\n",
      " |              The path to the json file.\n",
      " |\n",
      " |  from_peft_type(**kwargs) from builtins.type\n",
      " |      This method loads the configuration of your adapter model from a set of kwargs.\n",
      " |\n",
      " |      The appropriate configuration type is determined by the `peft_type` argument. If `peft_type` is not provided,\n",
      " |      the calling class type is instantiated.\n",
      " |\n",
      " |      Args:\n",
      " |          kwargs (configuration keyword arguments):\n",
      " |              Keyword arguments passed along to the configuration initialization.\n",
      " |\n",
      " |  from_pretrained(pretrained_model_name_or_path: str, subfolder: Optional[str] = None, **kwargs) from builtins.type\n",
      " |      This method loads the configuration of your adapter model from a directory.\n",
      " |\n",
      " |      Args:\n",
      " |          pretrained_model_name_or_path (`str`):\n",
      " |              The directory or the Hub repository id where the configuration is saved.\n",
      " |          kwargs (additional keyword arguments, *optional*):\n",
      " |              Additional keyword arguments passed along to the child class initialization.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from peft.config.PeftConfigMixin:\n",
      " |\n",
      " |  is_adaption_prompt\n",
      " |      Return True if this is an adaption prompt config.\n",
      " |\n",
      " |  is_prompt_learning\n",
      " |      Utility method to check if the configuration is for prompt learning.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from peft.config.PeftConfigMixin:\n",
      " |\n",
      " |  auto_mapping = None\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.utils.hub.PushToHubMixin:\n",
      " |\n",
      " |  push_to_hub(self, repo_id: str, use_temp_dir: Optional[bool] = None, commit_message: Optional[str] = None, private: Optional[bool] = None, token: Union[bool, str, NoneType] = None, max_shard_size: Union[int, str, NoneType] = '5GB', create_pr: bool = False, safe_serialization: bool = True, revision: Optional[str] = None, commit_description: Optional[str] = None, tags: Optional[list[str]] = None, **deprecated_kwargs) -> str\n",
      " |      Upload the {object_files} to the 🤗 Model Hub.\n",
      " |\n",
      " |      Parameters:\n",
      " |          repo_id (`str`):\n",
      " |              The name of the repository you want to push your {object} to. It should contain your organization name\n",
      " |              when pushing to a given organization.\n",
      " |          use_temp_dir (`bool`, *optional*):\n",
      " |              Whether or not to use a temporary directory to store the files saved before they are pushed to the Hub.\n",
      " |              Will default to `True` if there is no directory named like `repo_id`, `False` otherwise.\n",
      " |          commit_message (`str`, *optional*):\n",
      " |              Message to commit while pushing. Will default to `\"Upload {object}\"`.\n",
      " |          private (`bool`, *optional*):\n",
      " |              Whether to make the repo private. If `None` (default), the repo will be public unless the organization's default is private. This value is ignored if the repo already exists.\n",
      " |          token (`bool` or `str`, *optional*):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n",
      " |              when running `huggingface-cli login` (stored in `~/.huggingface`). Will default to `True` if `repo_url`\n",
      " |              is not specified.\n",
      " |          max_shard_size (`int` or `str`, *optional*, defaults to `\"5GB\"`):\n",
      " |              Only applicable for models. The maximum size for a checkpoint before being sharded. Checkpoints shard\n",
      " |              will then be each of size lower than this size. If expressed as a string, needs to be digits followed\n",
      " |              by a unit (like `\"5MB\"`). We default it to `\"5GB\"` so that users can easily load models on free-tier\n",
      " |              Google Colab instances without any CPU OOM issues.\n",
      " |          create_pr (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to create a PR with the uploaded files or directly commit.\n",
      " |          safe_serialization (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to convert the model weights in safetensors format for safer serialization.\n",
      " |          revision (`str`, *optional*):\n",
      " |              Branch to push the uploaded files to.\n",
      " |          commit_description (`str`, *optional*):\n",
      " |              The description of the commit that will be created\n",
      " |          tags (`List[str]`, *optional*):\n",
      " |              List of tags to push on the Hub.\n",
      " |\n",
      " |      Examples:\n",
      " |\n",
      " |      ```python\n",
      " |      from transformers import {object_class}\n",
      " |\n",
      " |      {object} = {object_class}.from_pretrained(\"google-bert/bert-base-cased\")\n",
      " |\n",
      " |      # Push the {object} to your namespace with the name \"my-finetuned-bert\".\n",
      " |      {object}.push_to_hub(\"my-finetuned-bert\")\n",
      " |\n",
      " |      # Push the {object} to an organization with the name \"my-finetuned-bert\".\n",
      " |      {object}.push_to_hub(\"huggingface/my-finetuned-bert\")\n",
      " |      ```\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from transformers.utils.hub.PushToHubMixin:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(LoraConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 112,899 || all params: 109,597,446 || trainable%: 0.1030\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train = ds['train'].train_test_split(test_size=0.1, seed=42)['test']  # 10% sample\n",
    "test = ds['test'].train_test_split(test_size=0.1, seed=42)['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['textID', 'text', 'label', 'label_text'],\n",
      "    num_rows: 2749\n",
      "}) Dataset({\n",
      "    features: ['textID', 'text', 'label', 'label_text'],\n",
      "    num_rows: 354\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = tokenizer(train['text'],return_tensors='pt',padding=True, truncation=True,max_length =512)\n",
    "tokenized_test = tokenizer(test['text'],return_tensors='pt',padding=True, truncation=True,max_length =512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SentimentDataset(tokenized_train, train['label'])\n",
    "val_dataset = SentimentDataset(tokenized_test, test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101,  2054,  4148,  2000,  1996, 29229,  1997,  6737,  1029,  8046,\n",
       "          2320,  2153,  1029,  1012,  1012,  1012,   102,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'labels': tensor(1)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "def compute_metrics(p):\n",
    "    preds, labels = p\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    return {\"accuracy\": accuracy_score(labels, preds)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2749"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2749"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_360626/1417597949.py:10: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir= \"lora-text-classification\",\n",
    "    learning_rate=1e-3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='688' max='688' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [688/688 00:22, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.890000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    trainer.train()\n",
    "except Exception as e:\n",
    "    print(f\"Error during raining: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='89' max='89' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [89/89 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6954167485237122,\n",
       " 'eval_accuracy': 0.7175141242937854,\n",
       " 'eval_runtime': 0.8248,\n",
       " 'eval_samples_per_second': 429.196,\n",
       " 'eval_steps_per_second': 107.905,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('lora_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.01, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=3, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=3, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.01, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=3, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=3, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): ModulesToSaveWrapper(\n",
       "    (original_module): Linear(in_features=768, out_features=3, bias=True)\n",
       "    (modules_to_save): ModuleDict(\n",
       "      (default): Linear(in_features=768, out_features=3, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_classification = BertForSequenceClassification.from_pretrained(\"./lora_model\", num_labels=3 )\n",
    "lora_classification.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: maybe someday I`ll find a book of yours on the bestsellers list? lol,awesome!\n",
      "Predicted: positive (0.98)\n",
      "Text: can`t go to bed  An am sooooo tired!\n",
      "Predicted: negative (0.85)\n",
      "Text: just woke up, no school today, we are free\n",
      "Predicted: neutral (0.90)\n"
     ]
    }
   ],
   "source": [
    "texts = [\"maybe someday I`ll find a book of yours on the bestsellers list? lol,awesome!\",\"can`t go to bed  An am sooooo tired!\",\"just woke up, no school today, we are free\"]\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "    outputs = lora_classification(**inputs)\n",
    "    probs = torch.softmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "\n",
    "for text, prob in zip(texts, probs):\n",
    "    pred_idx = np.argmax(prob)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Predicted: {label_names[pred_idx]} ({prob[pred_idx]:.2f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
