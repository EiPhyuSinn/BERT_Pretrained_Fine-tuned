{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eps/Documents/Projects/Bert(Training_with_new_dataset)/bert_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BertTokenizer,\n",
    "    AutoConfig, \n",
    "    AutoModelForSequenceClassification,\n",
    "    BertForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer)\n",
    "\n",
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['textID', 'text', 'label', 'label_text'],\n",
       "        num_rows: 27481\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['textID', 'text', 'label', 'label_text'],\n",
       "        num_rows: 3534\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"SetFit/tweet_sentiment_extraction\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping:\n",
      "   label label_text\n",
      "1      0   negative\n",
      "0      1    neutral\n",
      "6      2   positive\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['negative', 'neutral', 'positive']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.DataFrame(ds[\"train\"])\n",
    "\n",
    "label_mapping = train_df[[\"label\", \"label_text\"]].drop_duplicates().sort_values(\"label\")\n",
    "print(\"Label mapping:\")\n",
    "print(label_mapping)\n",
    "\n",
    "label_names = label_mapping[\"label_text\"].tolist()\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight True\n",
      "bert.embeddings.position_embeddings.weight True\n",
      "bert.embeddings.token_type_embeddings.weight True\n",
      "bert.embeddings.LayerNorm.weight True\n",
      "bert.embeddings.LayerNorm.bias True\n",
      "bert.encoder.layer.0.attention.self.query.weight True\n",
      "bert.encoder.layer.0.attention.self.query.bias True\n",
      "bert.encoder.layer.0.attention.self.key.weight True\n",
      "bert.encoder.layer.0.attention.self.key.bias True\n",
      "bert.encoder.layer.0.attention.self.value.weight True\n",
      "bert.encoder.layer.0.attention.self.value.bias True\n",
      "bert.encoder.layer.0.attention.output.dense.weight True\n",
      "bert.encoder.layer.0.attention.output.dense.bias True\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.0.intermediate.dense.weight True\n",
      "bert.encoder.layer.0.intermediate.dense.bias True\n",
      "bert.encoder.layer.0.output.dense.weight True\n",
      "bert.encoder.layer.0.output.dense.bias True\n",
      "bert.encoder.layer.0.output.LayerNorm.weight True\n",
      "bert.encoder.layer.0.output.LayerNorm.bias True\n",
      "bert.encoder.layer.1.attention.self.query.weight True\n",
      "bert.encoder.layer.1.attention.self.query.bias True\n",
      "bert.encoder.layer.1.attention.self.key.weight True\n",
      "bert.encoder.layer.1.attention.self.key.bias True\n",
      "bert.encoder.layer.1.attention.self.value.weight True\n",
      "bert.encoder.layer.1.attention.self.value.bias True\n",
      "bert.encoder.layer.1.attention.output.dense.weight True\n",
      "bert.encoder.layer.1.attention.output.dense.bias True\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.1.intermediate.dense.weight True\n",
      "bert.encoder.layer.1.intermediate.dense.bias True\n",
      "bert.encoder.layer.1.output.dense.weight True\n",
      "bert.encoder.layer.1.output.dense.bias True\n",
      "bert.encoder.layer.1.output.LayerNorm.weight True\n",
      "bert.encoder.layer.1.output.LayerNorm.bias True\n",
      "bert.encoder.layer.2.attention.self.query.weight True\n",
      "bert.encoder.layer.2.attention.self.query.bias True\n",
      "bert.encoder.layer.2.attention.self.key.weight True\n",
      "bert.encoder.layer.2.attention.self.key.bias True\n",
      "bert.encoder.layer.2.attention.self.value.weight True\n",
      "bert.encoder.layer.2.attention.self.value.bias True\n",
      "bert.encoder.layer.2.attention.output.dense.weight True\n",
      "bert.encoder.layer.2.attention.output.dense.bias True\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.2.intermediate.dense.weight True\n",
      "bert.encoder.layer.2.intermediate.dense.bias True\n",
      "bert.encoder.layer.2.output.dense.weight True\n",
      "bert.encoder.layer.2.output.dense.bias True\n",
      "bert.encoder.layer.2.output.LayerNorm.weight True\n",
      "bert.encoder.layer.2.output.LayerNorm.bias True\n",
      "bert.encoder.layer.3.attention.self.query.weight True\n",
      "bert.encoder.layer.3.attention.self.query.bias True\n",
      "bert.encoder.layer.3.attention.self.key.weight True\n",
      "bert.encoder.layer.3.attention.self.key.bias True\n",
      "bert.encoder.layer.3.attention.self.value.weight True\n",
      "bert.encoder.layer.3.attention.self.value.bias True\n",
      "bert.encoder.layer.3.attention.output.dense.weight True\n",
      "bert.encoder.layer.3.attention.output.dense.bias True\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.3.intermediate.dense.weight True\n",
      "bert.encoder.layer.3.intermediate.dense.bias True\n",
      "bert.encoder.layer.3.output.dense.weight True\n",
      "bert.encoder.layer.3.output.dense.bias True\n",
      "bert.encoder.layer.3.output.LayerNorm.weight True\n",
      "bert.encoder.layer.3.output.LayerNorm.bias True\n",
      "bert.encoder.layer.4.attention.self.query.weight True\n",
      "bert.encoder.layer.4.attention.self.query.bias True\n",
      "bert.encoder.layer.4.attention.self.key.weight True\n",
      "bert.encoder.layer.4.attention.self.key.bias True\n",
      "bert.encoder.layer.4.attention.self.value.weight True\n",
      "bert.encoder.layer.4.attention.self.value.bias True\n",
      "bert.encoder.layer.4.attention.output.dense.weight True\n",
      "bert.encoder.layer.4.attention.output.dense.bias True\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.4.intermediate.dense.weight True\n",
      "bert.encoder.layer.4.intermediate.dense.bias True\n",
      "bert.encoder.layer.4.output.dense.weight True\n",
      "bert.encoder.layer.4.output.dense.bias True\n",
      "bert.encoder.layer.4.output.LayerNorm.weight True\n",
      "bert.encoder.layer.4.output.LayerNorm.bias True\n",
      "bert.encoder.layer.5.attention.self.query.weight True\n",
      "bert.encoder.layer.5.attention.self.query.bias True\n",
      "bert.encoder.layer.5.attention.self.key.weight True\n",
      "bert.encoder.layer.5.attention.self.key.bias True\n",
      "bert.encoder.layer.5.attention.self.value.weight True\n",
      "bert.encoder.layer.5.attention.self.value.bias True\n",
      "bert.encoder.layer.5.attention.output.dense.weight True\n",
      "bert.encoder.layer.5.attention.output.dense.bias True\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.5.intermediate.dense.weight True\n",
      "bert.encoder.layer.5.intermediate.dense.bias True\n",
      "bert.encoder.layer.5.output.dense.weight True\n",
      "bert.encoder.layer.5.output.dense.bias True\n",
      "bert.encoder.layer.5.output.LayerNorm.weight True\n",
      "bert.encoder.layer.5.output.LayerNorm.bias True\n",
      "bert.encoder.layer.6.attention.self.query.weight True\n",
      "bert.encoder.layer.6.attention.self.query.bias True\n",
      "bert.encoder.layer.6.attention.self.key.weight True\n",
      "bert.encoder.layer.6.attention.self.key.bias True\n",
      "bert.encoder.layer.6.attention.self.value.weight True\n",
      "bert.encoder.layer.6.attention.self.value.bias True\n",
      "bert.encoder.layer.6.attention.output.dense.weight True\n",
      "bert.encoder.layer.6.attention.output.dense.bias True\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.6.intermediate.dense.weight True\n",
      "bert.encoder.layer.6.intermediate.dense.bias True\n",
      "bert.encoder.layer.6.output.dense.weight True\n",
      "bert.encoder.layer.6.output.dense.bias True\n",
      "bert.encoder.layer.6.output.LayerNorm.weight True\n",
      "bert.encoder.layer.6.output.LayerNorm.bias True\n",
      "bert.encoder.layer.7.attention.self.query.weight True\n",
      "bert.encoder.layer.7.attention.self.query.bias True\n",
      "bert.encoder.layer.7.attention.self.key.weight True\n",
      "bert.encoder.layer.7.attention.self.key.bias True\n",
      "bert.encoder.layer.7.attention.self.value.weight True\n",
      "bert.encoder.layer.7.attention.self.value.bias True\n",
      "bert.encoder.layer.7.attention.output.dense.weight True\n",
      "bert.encoder.layer.7.attention.output.dense.bias True\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.7.intermediate.dense.weight True\n",
      "bert.encoder.layer.7.intermediate.dense.bias True\n",
      "bert.encoder.layer.7.output.dense.weight True\n",
      "bert.encoder.layer.7.output.dense.bias True\n",
      "bert.encoder.layer.7.output.LayerNorm.weight True\n",
      "bert.encoder.layer.7.output.LayerNorm.bias True\n",
      "bert.encoder.layer.8.attention.self.query.weight True\n",
      "bert.encoder.layer.8.attention.self.query.bias True\n",
      "bert.encoder.layer.8.attention.self.key.weight True\n",
      "bert.encoder.layer.8.attention.self.key.bias True\n",
      "bert.encoder.layer.8.attention.self.value.weight True\n",
      "bert.encoder.layer.8.attention.self.value.bias True\n",
      "bert.encoder.layer.8.attention.output.dense.weight True\n",
      "bert.encoder.layer.8.attention.output.dense.bias True\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.8.intermediate.dense.weight True\n",
      "bert.encoder.layer.8.intermediate.dense.bias True\n",
      "bert.encoder.layer.8.output.dense.weight True\n",
      "bert.encoder.layer.8.output.dense.bias True\n",
      "bert.encoder.layer.8.output.LayerNorm.weight True\n",
      "bert.encoder.layer.8.output.LayerNorm.bias True\n",
      "bert.encoder.layer.9.attention.self.query.weight True\n",
      "bert.encoder.layer.9.attention.self.query.bias True\n",
      "bert.encoder.layer.9.attention.self.key.weight True\n",
      "bert.encoder.layer.9.attention.self.key.bias True\n",
      "bert.encoder.layer.9.attention.self.value.weight True\n",
      "bert.encoder.layer.9.attention.self.value.bias True\n",
      "bert.encoder.layer.9.attention.output.dense.weight True\n",
      "bert.encoder.layer.9.attention.output.dense.bias True\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.9.intermediate.dense.weight True\n",
      "bert.encoder.layer.9.intermediate.dense.bias True\n",
      "bert.encoder.layer.9.output.dense.weight True\n",
      "bert.encoder.layer.9.output.dense.bias True\n",
      "bert.encoder.layer.9.output.LayerNorm.weight True\n",
      "bert.encoder.layer.9.output.LayerNorm.bias True\n",
      "bert.encoder.layer.10.attention.self.query.weight True\n",
      "bert.encoder.layer.10.attention.self.query.bias True\n",
      "bert.encoder.layer.10.attention.self.key.weight True\n",
      "bert.encoder.layer.10.attention.self.key.bias True\n",
      "bert.encoder.layer.10.attention.self.value.weight True\n",
      "bert.encoder.layer.10.attention.self.value.bias True\n",
      "bert.encoder.layer.10.attention.output.dense.weight True\n",
      "bert.encoder.layer.10.attention.output.dense.bias True\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.10.intermediate.dense.weight True\n",
      "bert.encoder.layer.10.intermediate.dense.bias True\n",
      "bert.encoder.layer.10.output.dense.weight True\n",
      "bert.encoder.layer.10.output.dense.bias True\n",
      "bert.encoder.layer.10.output.LayerNorm.weight True\n",
      "bert.encoder.layer.10.output.LayerNorm.bias True\n",
      "bert.encoder.layer.11.attention.self.query.weight True\n",
      "bert.encoder.layer.11.attention.self.query.bias True\n",
      "bert.encoder.layer.11.attention.self.key.weight True\n",
      "bert.encoder.layer.11.attention.self.key.bias True\n",
      "bert.encoder.layer.11.attention.self.value.weight True\n",
      "bert.encoder.layer.11.attention.self.value.bias True\n",
      "bert.encoder.layer.11.attention.output.dense.weight True\n",
      "bert.encoder.layer.11.attention.output.dense.bias True\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.11.intermediate.dense.weight True\n",
      "bert.encoder.layer.11.intermediate.dense.bias True\n",
      "bert.encoder.layer.11.output.dense.weight True\n",
      "bert.encoder.layer.11.output.dense.bias True\n",
      "bert.encoder.layer.11.output.LayerNorm.weight True\n",
      "bert.encoder.layer.11.output.LayerNorm.bias True\n",
      "bert.pooler.dense.weight True\n",
      "bert.pooler.dense.bias True\n",
      "classifier.weight True\n",
      "classifier.bias True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name,param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.word_embeddings.weight False\n",
      "embeddings.position_embeddings.weight False\n",
      "embeddings.token_type_embeddings.weight False\n",
      "embeddings.LayerNorm.weight False\n",
      "embeddings.LayerNorm.bias False\n",
      "encoder.layer.0.attention.self.query.weight False\n",
      "encoder.layer.0.attention.self.query.bias False\n",
      "encoder.layer.0.attention.self.key.weight False\n",
      "encoder.layer.0.attention.self.key.bias False\n",
      "encoder.layer.0.attention.self.value.weight False\n",
      "encoder.layer.0.attention.self.value.bias False\n",
      "encoder.layer.0.attention.output.dense.weight False\n",
      "encoder.layer.0.attention.output.dense.bias False\n",
      "encoder.layer.0.attention.output.LayerNorm.weight False\n",
      "encoder.layer.0.attention.output.LayerNorm.bias False\n",
      "encoder.layer.0.intermediate.dense.weight False\n",
      "encoder.layer.0.intermediate.dense.bias False\n",
      "encoder.layer.0.output.dense.weight False\n",
      "encoder.layer.0.output.dense.bias False\n",
      "encoder.layer.0.output.LayerNorm.weight False\n",
      "encoder.layer.0.output.LayerNorm.bias False\n",
      "encoder.layer.1.attention.self.query.weight False\n",
      "encoder.layer.1.attention.self.query.bias False\n",
      "encoder.layer.1.attention.self.key.weight False\n",
      "encoder.layer.1.attention.self.key.bias False\n",
      "encoder.layer.1.attention.self.value.weight False\n",
      "encoder.layer.1.attention.self.value.bias False\n",
      "encoder.layer.1.attention.output.dense.weight False\n",
      "encoder.layer.1.attention.output.dense.bias False\n",
      "encoder.layer.1.attention.output.LayerNorm.weight False\n",
      "encoder.layer.1.attention.output.LayerNorm.bias False\n",
      "encoder.layer.1.intermediate.dense.weight False\n",
      "encoder.layer.1.intermediate.dense.bias False\n",
      "encoder.layer.1.output.dense.weight False\n",
      "encoder.layer.1.output.dense.bias False\n",
      "encoder.layer.1.output.LayerNorm.weight False\n",
      "encoder.layer.1.output.LayerNorm.bias False\n",
      "encoder.layer.2.attention.self.query.weight False\n",
      "encoder.layer.2.attention.self.query.bias False\n",
      "encoder.layer.2.attention.self.key.weight False\n",
      "encoder.layer.2.attention.self.key.bias False\n",
      "encoder.layer.2.attention.self.value.weight False\n",
      "encoder.layer.2.attention.self.value.bias False\n",
      "encoder.layer.2.attention.output.dense.weight False\n",
      "encoder.layer.2.attention.output.dense.bias False\n",
      "encoder.layer.2.attention.output.LayerNorm.weight False\n",
      "encoder.layer.2.attention.output.LayerNorm.bias False\n",
      "encoder.layer.2.intermediate.dense.weight False\n",
      "encoder.layer.2.intermediate.dense.bias False\n",
      "encoder.layer.2.output.dense.weight False\n",
      "encoder.layer.2.output.dense.bias False\n",
      "encoder.layer.2.output.LayerNorm.weight False\n",
      "encoder.layer.2.output.LayerNorm.bias False\n",
      "encoder.layer.3.attention.self.query.weight False\n",
      "encoder.layer.3.attention.self.query.bias False\n",
      "encoder.layer.3.attention.self.key.weight False\n",
      "encoder.layer.3.attention.self.key.bias False\n",
      "encoder.layer.3.attention.self.value.weight False\n",
      "encoder.layer.3.attention.self.value.bias False\n",
      "encoder.layer.3.attention.output.dense.weight False\n",
      "encoder.layer.3.attention.output.dense.bias False\n",
      "encoder.layer.3.attention.output.LayerNorm.weight False\n",
      "encoder.layer.3.attention.output.LayerNorm.bias False\n",
      "encoder.layer.3.intermediate.dense.weight False\n",
      "encoder.layer.3.intermediate.dense.bias False\n",
      "encoder.layer.3.output.dense.weight False\n",
      "encoder.layer.3.output.dense.bias False\n",
      "encoder.layer.3.output.LayerNorm.weight False\n",
      "encoder.layer.3.output.LayerNorm.bias False\n",
      "encoder.layer.4.attention.self.query.weight False\n",
      "encoder.layer.4.attention.self.query.bias False\n",
      "encoder.layer.4.attention.self.key.weight False\n",
      "encoder.layer.4.attention.self.key.bias False\n",
      "encoder.layer.4.attention.self.value.weight False\n",
      "encoder.layer.4.attention.self.value.bias False\n",
      "encoder.layer.4.attention.output.dense.weight False\n",
      "encoder.layer.4.attention.output.dense.bias False\n",
      "encoder.layer.4.attention.output.LayerNorm.weight False\n",
      "encoder.layer.4.attention.output.LayerNorm.bias False\n",
      "encoder.layer.4.intermediate.dense.weight False\n",
      "encoder.layer.4.intermediate.dense.bias False\n",
      "encoder.layer.4.output.dense.weight False\n",
      "encoder.layer.4.output.dense.bias False\n",
      "encoder.layer.4.output.LayerNorm.weight False\n",
      "encoder.layer.4.output.LayerNorm.bias False\n",
      "encoder.layer.5.attention.self.query.weight False\n",
      "encoder.layer.5.attention.self.query.bias False\n",
      "encoder.layer.5.attention.self.key.weight False\n",
      "encoder.layer.5.attention.self.key.bias False\n",
      "encoder.layer.5.attention.self.value.weight False\n",
      "encoder.layer.5.attention.self.value.bias False\n",
      "encoder.layer.5.attention.output.dense.weight False\n",
      "encoder.layer.5.attention.output.dense.bias False\n",
      "encoder.layer.5.attention.output.LayerNorm.weight False\n",
      "encoder.layer.5.attention.output.LayerNorm.bias False\n",
      "encoder.layer.5.intermediate.dense.weight False\n",
      "encoder.layer.5.intermediate.dense.bias False\n",
      "encoder.layer.5.output.dense.weight False\n",
      "encoder.layer.5.output.dense.bias False\n",
      "encoder.layer.5.output.LayerNorm.weight False\n",
      "encoder.layer.5.output.LayerNorm.bias False\n",
      "encoder.layer.6.attention.self.query.weight False\n",
      "encoder.layer.6.attention.self.query.bias False\n",
      "encoder.layer.6.attention.self.key.weight False\n",
      "encoder.layer.6.attention.self.key.bias False\n",
      "encoder.layer.6.attention.self.value.weight False\n",
      "encoder.layer.6.attention.self.value.bias False\n",
      "encoder.layer.6.attention.output.dense.weight False\n",
      "encoder.layer.6.attention.output.dense.bias False\n",
      "encoder.layer.6.attention.output.LayerNorm.weight False\n",
      "encoder.layer.6.attention.output.LayerNorm.bias False\n",
      "encoder.layer.6.intermediate.dense.weight False\n",
      "encoder.layer.6.intermediate.dense.bias False\n",
      "encoder.layer.6.output.dense.weight False\n",
      "encoder.layer.6.output.dense.bias False\n",
      "encoder.layer.6.output.LayerNorm.weight False\n",
      "encoder.layer.6.output.LayerNorm.bias False\n",
      "encoder.layer.7.attention.self.query.weight False\n",
      "encoder.layer.7.attention.self.query.bias False\n",
      "encoder.layer.7.attention.self.key.weight False\n",
      "encoder.layer.7.attention.self.key.bias False\n",
      "encoder.layer.7.attention.self.value.weight False\n",
      "encoder.layer.7.attention.self.value.bias False\n",
      "encoder.layer.7.attention.output.dense.weight False\n",
      "encoder.layer.7.attention.output.dense.bias False\n",
      "encoder.layer.7.attention.output.LayerNorm.weight False\n",
      "encoder.layer.7.attention.output.LayerNorm.bias False\n",
      "encoder.layer.7.intermediate.dense.weight False\n",
      "encoder.layer.7.intermediate.dense.bias False\n",
      "encoder.layer.7.output.dense.weight False\n",
      "encoder.layer.7.output.dense.bias False\n",
      "encoder.layer.7.output.LayerNorm.weight False\n",
      "encoder.layer.7.output.LayerNorm.bias False\n",
      "encoder.layer.8.attention.self.query.weight False\n",
      "encoder.layer.8.attention.self.query.bias False\n",
      "encoder.layer.8.attention.self.key.weight False\n",
      "encoder.layer.8.attention.self.key.bias False\n",
      "encoder.layer.8.attention.self.value.weight False\n",
      "encoder.layer.8.attention.self.value.bias False\n",
      "encoder.layer.8.attention.output.dense.weight False\n",
      "encoder.layer.8.attention.output.dense.bias False\n",
      "encoder.layer.8.attention.output.LayerNorm.weight False\n",
      "encoder.layer.8.attention.output.LayerNorm.bias False\n",
      "encoder.layer.8.intermediate.dense.weight False\n",
      "encoder.layer.8.intermediate.dense.bias False\n",
      "encoder.layer.8.output.dense.weight False\n",
      "encoder.layer.8.output.dense.bias False\n",
      "encoder.layer.8.output.LayerNorm.weight False\n",
      "encoder.layer.8.output.LayerNorm.bias False\n",
      "encoder.layer.9.attention.self.query.weight False\n",
      "encoder.layer.9.attention.self.query.bias False\n",
      "encoder.layer.9.attention.self.key.weight False\n",
      "encoder.layer.9.attention.self.key.bias False\n",
      "encoder.layer.9.attention.self.value.weight False\n",
      "encoder.layer.9.attention.self.value.bias False\n",
      "encoder.layer.9.attention.output.dense.weight False\n",
      "encoder.layer.9.attention.output.dense.bias False\n",
      "encoder.layer.9.attention.output.LayerNorm.weight False\n",
      "encoder.layer.9.attention.output.LayerNorm.bias False\n",
      "encoder.layer.9.intermediate.dense.weight False\n",
      "encoder.layer.9.intermediate.dense.bias False\n",
      "encoder.layer.9.output.dense.weight False\n",
      "encoder.layer.9.output.dense.bias False\n",
      "encoder.layer.9.output.LayerNorm.weight False\n",
      "encoder.layer.9.output.LayerNorm.bias False\n",
      "encoder.layer.10.attention.self.query.weight False\n",
      "encoder.layer.10.attention.self.query.bias False\n",
      "encoder.layer.10.attention.self.key.weight False\n",
      "encoder.layer.10.attention.self.key.bias False\n",
      "encoder.layer.10.attention.self.value.weight False\n",
      "encoder.layer.10.attention.self.value.bias False\n",
      "encoder.layer.10.attention.output.dense.weight False\n",
      "encoder.layer.10.attention.output.dense.bias False\n",
      "encoder.layer.10.attention.output.LayerNorm.weight False\n",
      "encoder.layer.10.attention.output.LayerNorm.bias False\n",
      "encoder.layer.10.intermediate.dense.weight False\n",
      "encoder.layer.10.intermediate.dense.bias False\n",
      "encoder.layer.10.output.dense.weight False\n",
      "encoder.layer.10.output.dense.bias False\n",
      "encoder.layer.10.output.LayerNorm.weight False\n",
      "encoder.layer.10.output.LayerNorm.bias False\n",
      "encoder.layer.11.attention.self.query.weight False\n",
      "encoder.layer.11.attention.self.query.bias False\n",
      "encoder.layer.11.attention.self.key.weight False\n",
      "encoder.layer.11.attention.self.key.bias False\n",
      "encoder.layer.11.attention.self.value.weight False\n",
      "encoder.layer.11.attention.self.value.bias False\n",
      "encoder.layer.11.attention.output.dense.weight False\n",
      "encoder.layer.11.attention.output.dense.bias False\n",
      "encoder.layer.11.attention.output.LayerNorm.weight False\n",
      "encoder.layer.11.attention.output.LayerNorm.bias False\n",
      "encoder.layer.11.intermediate.dense.weight False\n",
      "encoder.layer.11.intermediate.dense.bias False\n",
      "encoder.layer.11.output.dense.weight False\n",
      "encoder.layer.11.output.dense.bias False\n",
      "encoder.layer.11.output.LayerNorm.weight False\n",
      "encoder.layer.11.output.LayerNorm.bias False\n",
      "pooler.dense.weight True\n",
      "pooler.dense.bias True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.base_model.named_parameters():\n",
    "    param.requires_grad = True if \"pooler\" in name else False\n",
    "    print(name,param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(dataset):\n",
    "    return tokenizer(dataset['text'],truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 27481/27481 [00:04<00:00, 5717.31 examples/s]\n",
      "Map: 100%|██████████| 3534/3534 [00:00<00:00, 5679.20 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_ds = ds.map(tokenize,batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['textID', 'text', 'label', 'label_text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 27481\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['textID', 'text', 'label', 'label_text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3534\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "def compute_metrics(p):\n",
    "    preds, labels = p\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    return {\"accuracy\": accuracy_score(labels, preds)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_428816/2293576099.py:11: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    save_steps=100,\n",
    "    eval_steps=100\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_ds['train'],\n",
    "    eval_dataset=tokenized_ds['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1718' max='1718' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1718/1718 00:48, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.876500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.852600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.857900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.851400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.855300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.840200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.846100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.828900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.845700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.835200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.822600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.831900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.818000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.847200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.838800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.852500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.830700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    trainer.train()\n",
    "except Exception as e:\n",
    "    print(f\"Error during raining: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.6397849462365591}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predictions = trainer.predict(tokenized_ds[\"test\"])\n",
    "\n",
    "logits = predictions.predictions\n",
    "labels = predictions.label_ids\n",
    "\n",
    "metrics = compute_metrics((logits, labels))\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('transfer_learning_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = BertForSequenceClassification.from_pretrained(\"./transfer_learning_model\")\n",
    "model1.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7622, -0.1409,  1.8675],\n",
       "        [ 1.5190, -0.0333, -1.8211],\n",
       "        [-0.6380,  0.5900, -0.1299]], device='cuda:0')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\"maybe someday I`ll find a book of yours on the bestsellers list? lol,awesome!\",\"can`t go to bed  An am sooooo tired!\",\"just woke up, no school today, we are free\"]\n",
    "inputs = tokenizer(texts,padding=True,truncation=True, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model1(**inputs)\n",
    "    logits  = outputs.logits\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02285041, 0.11561893, 0.86153066],\n",
       "       [0.8017957 , 0.16979383, 0.02841046],\n",
       "       [0.164569  , 0.56190395, 0.27352706]], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "probs = torch.softmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 1])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maybe someday I`ll find a book of yours on the bestsellers list? lol,awesome!: positive (0.86)\n",
      "can`t go to bed  An am sooooo tired!: negative (0.80)\n",
      "just woke up, no school today, we are free: neutral (0.56)\n"
     ]
    }
   ],
   "source": [
    "for text, pred_idx, prob_dist in zip(texts, predictions, probs):\n",
    "    label = label_names[pred_idx]\n",
    "    prob = prob_dist.max()\n",
    "    print(f'{text}: {label} ({prob:.2f})')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
